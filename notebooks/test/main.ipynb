{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "[1,2,3,0,0,0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.dataset_readers.vctk import VCTK\n",
    "import random\n",
    "import ujson as json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "DATASET_ROOT = Path('/home/lnan6257/work/dataset/speech/VCTK-Corpus')\n",
    "PROJECT_ROOT = Path('/home/lnan6257/work/allennlp-speech')\n",
    "serilization_dir = Path('./serilization_dir')\n",
    "serilization_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger():\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    h = logging.FileHandler(str(serilization_dir / 'log.out'), mode='w')\n",
    "    logger.addHandler(h)\n",
    "\n",
    "    return logger\n",
    "logger = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for generate tiny dataset from full\n",
    "\"\"\"\n",
    "dataset_train = DATASET_ROOT / 'generated' / 'dataset_list.train'\n",
    "dataset_dev = DATASET_ROOT / 'generated' / 'dataset_list.dev'\n",
    "dataset_test = DATASET_ROOT / 'generated' / 'dataset_list.test'\n",
    "def generate_small(dataset):\n",
    "    with open(dataset) as fh:\n",
    "        data = json.load(fh)\n",
    "    small_dataset_path = Path(str(dataset) + '.small')\n",
    "    size = len(data)\n",
    "    small_size = max(100, int(size/50))\n",
    "    random.shuffle(data)\n",
    "    with open(small_dataset_path, 'w') as fh:\n",
    "        json.dump(data[:small_size], fh)\n",
    "#generate_small(dataset_train)\n",
    "generate_small(dataset_dev)\n",
    "generate_small(dataset_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static for train\n",
      "size = 685\n",
      "----------------------------------------------------------------------------------------------------\n",
      "static for dev\n",
      "size = 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "static for test\n",
      "size = 103\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_train_path = DATASET_ROOT / 'generated' / 'dataset_list.train.small'\n",
    "dataset_dev_path = DATASET_ROOT / 'generated' / 'dataset_list.dev.small'\n",
    "dataset_test_path = DATASET_ROOT / 'generated' / 'dataset_list.test.small'\n",
    "with open(dataset_train_path) as file:\n",
    "    dataset_train = json.load(file)\n",
    "with open(dataset_dev_path) as file:\n",
    "    dataset_dev = json.load(file)\n",
    "with open(dataset_test_path) as file:\n",
    "    dataset_test = json.load(file)\n",
    "def show_static(name, dataset):\n",
    "    print(f'static for {name}')\n",
    "    print(f'size = {len(dataset)}')\n",
    "    print('-' * 100)\n",
    "show_static('train', dataset_train)\n",
    "show_static('dev', dataset_dev)\n",
    "show_static('test', dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "params_dict = {\n",
    "    'dataset_reader':{\n",
    "        'type': 'vctk',\n",
    "        'cache_path': DATASET_ROOT / 'data-small.hdf5',\n",
    "        'feature_name': 'mag',\n",
    "        'audio_transformer': {\n",
    "            'type': 'mag'\n",
    "        }\n",
    "    },\n",
    "    'train_data_path': dataset_train_path,\n",
    "    'validation_data_path': dataset_dev_path,\n",
    "    'test_data_path': dataset_test_path,\n",
    "}\n",
    "params = Params(params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of src.dataset_readers.vctk failed: Traceback (most recent call last):\n",
      "  File \"/home/lnan6257/work/allennlp-speech/myenv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/lnan6257/work/allennlp-speech/myenv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 368, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/lnan6257/work/allennlp-speech/myenv/lib/python3.6/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/lnan6257/work/allennlp-speech/myenv/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 608, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\n",
      "  File \"/home/lnan6257/work/allennlp-speech/src/dataset_readers/vctk.py\", line 20, in <module>\n",
      "    class VCTK(SpeechDatasetReader):\n",
      "  File \"/home/lnan6257/work/allennlp-speech/myenv/lib/python3.6/site-packages/allennlp/common/registrable.py\", line 49, in add_subclass_to_registry\n",
      "    raise ConfigurationError(message)\n",
      "allennlp.common.checks.ConfigurationError: 'Cannot register vctk as DatasetReader; name already in use for VCTK'\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {} and extras {}\n"
     ]
    },
    {
     "ename": "ConfigurationError",
     "evalue": "'key \"type\" is required at location \"dataset_reader.\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/work/allennlp-speech/myenv/lib/python3.6/site-packages/allennlp/common/params.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d5ffe1e7c700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d5ffe1e7c700>\u001b[0m in \u001b[0;36mdatasets_from_params\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLoad\u001b[0m \u001b[0mall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdataset_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset_reader'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_dataset_reader_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_dataset_reader\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/allennlp-speech/myenv/lib/python3.6/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    229\u001b[0m             choice = params.pop_choice(\"type\",\n\u001b[1;32m    230\u001b[0m                                        \u001b[0mchoices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_registrable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                                        default_to_first_choice=default_to_first_choice)\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0msubclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistered_subclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/allennlp-speech/myenv/lib/python3.6/site-packages/allennlp/common/params.py\u001b[0m in \u001b[0;36mpop_choice\u001b[0;34m(self, key, choices, default_to_first_choice)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m    254\u001b[0m         \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdefault_to_first_choice\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mkey_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/allennlp-speech/myenv/lib/python3.6/site-packages/allennlp/common/params.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key \\\"{}\\\" is required at location \\\"{}\\\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: 'key \"type\" is required at location \"dataset_reader.\"'"
     ]
    }
   ],
   "source": [
    "from allennlp.data import DatasetReader\n",
    "def datasets_from_params(params: Params):\n",
    "    \"\"\"\n",
    "    Load all the datasets specified by the config.\n",
    "    \"\"\"\n",
    "    dataset_reader = DatasetReader.from_params(params.get('dataset_reader'))\n",
    "    validation_dataset_reader_params = params.get(\"validation_dataset_reader\", None)\n",
    "\n",
    "    validation_and_test_dataset_reader: DatasetReader = dataset_reader\n",
    "    if validation_dataset_reader_params is not None:\n",
    "        logger.info(\"Using a separate dataset reader to load validation and test data.\")\n",
    "        validation_and_test_dataset_reader = DatasetReader.from_params(validation_dataset_reader_params)\n",
    "\n",
    "    train_data_path = params.get('train_data_path')\n",
    "    logger.info(\"Reading training data from %s\", train_data_path)\n",
    "    train_data = dataset_reader.read(train_data_path)\n",
    "\n",
    "    datasets: Dict[str, Iterable[Instance]] = {\"train\": train_data}\n",
    "\n",
    "    validation_data_path = params.get('validation_data_path', None)\n",
    "    if validation_data_path is not None:\n",
    "        logger.info(\"Reading validation data from %s\", validation_data_path)\n",
    "        validation_data = validation_and_test_dataset_reader.read(validation_data_path)\n",
    "        datasets[\"validation\"] = validation_data\n",
    "\n",
    "    test_data_path = params.pop(\"test_data_path\", None)\n",
    "    if test_data_path is not None:\n",
    "        logger.info(\"Reading test data from %s\", test_data_path)\n",
    "        test_data = validation_and_test_dataset_reader.read(test_data_path)\n",
    "        datasets[\"test\"] = test_data\n",
    "\n",
    "    return datasets\n",
    "datasets = datasets_from_params(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
