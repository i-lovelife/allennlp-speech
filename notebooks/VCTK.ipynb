{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lnan6257/work/dataset/speech/VCTK-Corpus/wav48/p345/p345_046.wav\n"
     ]
    }
   ],
   "source": [
    "x = \"/home/lnan6257/work/dataset/speech/VCTK-Corpus/wav48/p345/p345_046.wav\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from allennlp.training.trainer import TensorboardWriter\n",
    "\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from external.model import Encoder, Decoder, SpeakerClassifier\n",
    "from external.model_speech import DeepSpeech\n",
    "# Set data root\n",
    "VCTK_ROOT = Path('/home/nanlh/dataset/speech/VCTK-Corpus')\n",
    "VCTK_ROOTs = str(VCTK_ROOT)\n",
    "DEBUG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,0,0,0,1,2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple('Config', 'serialization_dir cache_path batch_size num_workers train_steps valid_steps lr summary_interval')\n",
    "config_train = Config(\n",
    "    serialization_dir = './vctk_train/',\n",
    "    cache_path = 'data.hdf5',\n",
    "    batch_size = 32,\n",
    "    num_workers = 0,\n",
    "    train_steps = 2000,\n",
    "    valid_steps = 500,\n",
    "    lr = 0.002,\n",
    "    summary_interval = 100\n",
    ")\n",
    "config_debug = Config(\n",
    "    serialization_dir = './vctk_debug/',\n",
    "    cache_path = 'data.hdf5',\n",
    "    batch_size = 2,\n",
    "    num_workers = 0,\n",
    "    train_steps = 2,\n",
    "    valid_steps = 1,\n",
    "    lr = 0.002,\n",
    "    summary_interval = 1\n",
    ")\n",
    "if DEBUG:\n",
    "    config = config_debug\n",
    "else:\n",
    "    config = config_train\n",
    "    \n",
    "serialization_dir = Path(config.serialization_dir)\n",
    "serialization_dir.mkdir(exist_ok=True)\n",
    "tensorboard_dir = serialization_dir / 'log/'\n",
    "if tensorboard_dir.exists():\n",
    "    shutil.rmtree(tensorboard_dir)\n",
    "cache_path = VCTK_ROOT / config.cache_path\n",
    "log_path = serialization_dir / 'log.txt'\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(DEBUG):\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    if DEBUG:\n",
    "        ch = logging.StreamHandler(sys.stdout)\n",
    "        logger.addHandler(ch)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)\n",
    "    h = logging.FileHandler(log_path, mode='w')\n",
    "    logger.addHandler(h)\n",
    "\n",
    "    return logger\n",
    "logger = init_logger(DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ID  AGE  GENDER  ACCENTS  REGION\n",
    "225  23  F    English    Southern  England\n",
    "\"\"\"\n",
    "\n",
    "SPEAKER = VCTK_ROOT/'speaker-info.txt'\n",
    "with open(SPEAKER) as f:\n",
    "    speakers = f.readlines()\n",
    "def reform(lst):\n",
    "    lst = [x for x in lst if x != '']\n",
    "    ret = {'id': int(lst[0]),\n",
    "           'age': int(lst[1]),\n",
    "           'sex': 1 if lst[2] == 'F' else 0,\n",
    "           'accents': lst[3]\n",
    "          }\n",
    "    return ret\n",
    "\n",
    "speakers = [reform(x.strip().split(' ')) for x in speakers[1:]]\n",
    "speakers = [speaker for speaker in speakers if speaker['id'] != 315]\n",
    "print(speakers[:2])\n",
    "\n",
    "# Split speaker by sex, make sure balance in train, dev and test\n",
    "girls = [x for x in speakers if x['sex'] == 1]\n",
    "boys = [x for x in speakers if x['sex'] == 0]\n",
    "\n",
    "def split_data(data):\n",
    "    train_len = int(len(data) * 0.8)\n",
    "    dev_len = int(len(data) * 0.1)\n",
    "    return (data[:train_len], data[train_len: train_len + dev_len], data[train_len + dev_len:])\n",
    "\n",
    "girls_train, girls_dev, girls_test = split_data(girls)\n",
    "boys_train, boys_dev, boys_test = split_data(boys)\n",
    "\n",
    "print()\n",
    "print('girls')\n",
    "print(f'train: {len(girls_train)}, dev:{len(girls_dev)}, test:{len(girls_test)}')\n",
    "print('boys')\n",
    "print(f'train: {len(boys_train)}, dev:{len(boys_dev)}, test:{len(boys_test)}')\n",
    "\n",
    "# Generate example\n",
    "def add_example(examples, speakers):\n",
    "    for speaker in speakers:\n",
    "        idx = speaker['id']\n",
    "        files = glob.glob(str(VCTK_ROOTs) + '/wav48/p' + str(idx) + '/*.wav')\n",
    "        for file in files:\n",
    "            txt_file = file.replace('wav48', 'txt').replace('wav', 'txt')\n",
    "            example = {}\n",
    "            example['sex_label'] = speaker['sex']\n",
    "            example['audio_file'] = file\n",
    "            example['txt_file'] = txt_file\n",
    "            \n",
    "            example['speaker_chapter'] = file.split('.')[0].split('_')[1]\n",
    "            example['speaker_id'] = str(idx)\n",
    "            \n",
    "            examples.append(example)\n",
    "\n",
    "example_train, example_dev, example_test = [], [], []\n",
    "add_example(example_train, girls_train)\n",
    "add_example(example_train, boys_train)\n",
    "add_example(example_dev, girls_dev)\n",
    "add_example(example_dev, boys_dev)\n",
    "add_example(example_test, girls_test)\n",
    "add_example(example_test, boys_test)\n",
    "\n",
    "print()\n",
    "print(example_train[:2])\n",
    "print()\n",
    "print('examples counts')\n",
    "print(f'train: {len(example_train)}, dev: {len(example_dev)}, test: {len(example_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = example_train[0]['audio_file']\n",
    "ipd.Audio(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = example_train[0]['txt_file']\n",
    "with open(example_text) as fh:\n",
    "    doc = fh.readline()\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToSpec(object):\n",
    "    \"\"\"Returns normalized log(magnitude) from `sound_file`.\n",
    "    Args:\n",
    "    sound_file: A string. The full path of a sound file.\n",
    "    Returns:\n",
    "    mag: A 2d array of shape (T, 1+n_fft/2) <- Transposed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sr = 16000,\n",
    "                 frame_shift=0.0125,\n",
    "                 frame_length=0.05,\n",
    "                 trim=True,\n",
    "                 preemphasis=0.97,\n",
    "                 n_fft=1024,\n",
    "                 max_db=100,\n",
    "                 ref_db=20,\n",
    "                ):\n",
    "        self.sr = sr\n",
    "        self.frame_shift = frame_shift\n",
    "        self.frame_length = frame_length\n",
    "        self.trim = trim\n",
    "        self.preemphasis = preemphasis\n",
    "        self.n_fft = n_fft\n",
    "        self.max_db = max_db\n",
    "        self.ref_db = ref_db\n",
    "    \n",
    "    def __call__(self, audio_file):\n",
    "        mag = self.load_and_transform(audio_file)\n",
    "        spec = torch.tensor(mag)\n",
    "        return spec\n",
    "    \n",
    "    def load_and_transform(self, fpath):\n",
    "\n",
    "        # Loading sound file\n",
    "        y, sr = librosa.load(fpath, sr=self.sr)\n",
    "        if self.sr is None:\n",
    "            self.sr = sr\n",
    "\n",
    "        hop_length = int(sr * self.frame_shift)\n",
    "        win_length = int(sr * self.frame_length)\n",
    "        # Trimming\n",
    "        if self.trim:\n",
    "            y, _ = librosa.effects.trim(y)\n",
    "\n",
    "        # Preemphasis\n",
    "        y = np.append(y[0], y[1:] - self.preemphasis * y[:-1])\n",
    "\n",
    "        # stft\n",
    "        linear = librosa.stft(y=y,\n",
    "                            n_fft=self.n_fft,\n",
    "                            hop_length=hop_length,\n",
    "                            win_length=win_length)\n",
    "\n",
    "        # magnitude spectrogram\n",
    "        mag = np.abs(linear)  # (1+n_fft//2, T)\n",
    "        mag = 20 * np.log10(np.maximum(1e-5, mag))\n",
    "        mag = np.clip((mag - self.ref_db + self.max_db) / self.max_db, 1e-8, 1)\n",
    "        mag = mag.T.astype(np.float32)  # (T, 1+n_fft//2)\n",
    "\n",
    "        return mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self.labels =[\n",
    "          \"#\",\n",
    "          \"'\",\n",
    "          \"A\",\n",
    "          \"B\",\n",
    "          \"C\",\n",
    "          \"D\",\n",
    "          \"E\",\n",
    "          \"F\",\n",
    "          \"G\",\n",
    "          \"H\",\n",
    "          \"I\",\n",
    "          \"J\",\n",
    "          \"K\",\n",
    "          \"L\",\n",
    "          \"M\",\n",
    "          \"N\",\n",
    "          \"O\",\n",
    "          \"P\",\n",
    "          \"Q\",\n",
    "          \"R\",\n",
    "          \"S\",\n",
    "          \"T\",\n",
    "          \"U\",\n",
    "          \"V\",\n",
    "          \"W\",\n",
    "          \"X\",\n",
    "          \"Y\",\n",
    "          \"Z\",\n",
    "          \" \"\n",
    "        ]\n",
    "        self.labels_map = {ch:idx for idx, ch in enumerate(self.labels)}\n",
    "    def get_idx(self, ch):\n",
    "        if ch not in self.labels:\n",
    "            return len(self.labels) - 1\n",
    "        return self.labels_map[ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeTxt(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        with open(filename) as fh:\n",
    "            txt = fh.readline()\n",
    "        all_ch = txt.strip().replace('\\n', '')\n",
    "        return [self.vocab.get_idx(x.upper()) for x in all_ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = VCTK_ROOT / 'txt/p225/p225_001.txt'\n",
    "with open(txt_file) as fh:\n",
    "    line = fh.readline()\n",
    "    line = line.strip().replace('\\n', '').upper()\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCTK(Dataset):\n",
    "    def __init__(self, examples, cache_path=None, transform_audio=ToSpec(), transform_txt=NormalizeTxt(Vocab())):\n",
    "        self.examples = examples\n",
    "        self.transform_audio = transform_audio\n",
    "        self.transform_txt = transform_txt\n",
    "        self.in_cpu = [False] * len(self.examples)\n",
    "        if cache_path is not None:\n",
    "            self.cache_file = h5py.File(cache_path, 'a')\n",
    "        else:\n",
    "            self.cache_file = None\n",
    "        self.cache_all()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        if self.in_cpu[idx]:\n",
    "            return example\n",
    "        # None-cache attr\n",
    "        example['txt'] = self.transform_txt(example['txt_file'])\n",
    "        example['sex_label'] = torch.tensor([example['sex_label']])\n",
    "        \n",
    "        # Cache attr\n",
    "        if self.cache_file:\n",
    "            speaker_id = example['speaker_id']\n",
    "            speaker_chapter = example['speaker_chapter']\n",
    "            loc = f'{speaker_id}/{speaker_chapter}/mag'\n",
    "            \n",
    "            if loc in self.cache_file:\n",
    "                logging.debug('read from cache')\n",
    "                example['feature'] = self.cache_file[loc].value\n",
    "            else:\n",
    "                logging.debug('process from raw and store it in cache')\n",
    "                mag = self.transform_audio(example['audio_file'])\n",
    "                self.cache_file.create_dataset(loc, data=mag, dtype=np.float32)\n",
    "                example['feature'] = mag\n",
    "        else:\n",
    "            logging.debug('dont use cache')\n",
    "            example['feature'] = self.transform_audio(example['audio_file'])\n",
    "        example['feature'] = torch.FloatTensor(example['feature'])\n",
    "        \n",
    "        self.examples[idx] = example\n",
    "        self.in_cpu[idx] = True\n",
    "        return example\n",
    "    \n",
    "    def cache_all(self):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=1)\n",
    "        for x in tqdm(dataloader, desc='cache data'):\n",
    "            pass\n",
    "        if self.cache_file:\n",
    "            self.cache_file.close()\n",
    "            self.cache_file = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    vctk_train = VCTK(examples=example_train[:10], cache_path=cache_path)\n",
    "    vctk_dev = VCTK(examples=example_dev[:10], cache_path=cache_path)\n",
    "else:\n",
    "    vctk_train = VCTK(examples=example_train, cache_path=cache_path)\n",
    "    vctk_dev = VCTK(examples=example_dev, cache_path=cache_path)\n",
    "\n",
    "print(len(vctk_train))\n",
    "print(len(vctk_dev))\n",
    "print(vctk_train[0]['feature'].size())\n",
    "print(vctk_train[0]['sex_label'].size())\n",
    "print(vctk_train[0]['txt'])\n",
    "print(vctk_train[0]['feature'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "longest_target = max(vctk_train, key=lambda x: len(x['txt']))\n",
    "print(f'longest_target: len={len(longest_target[\"txt\"])} in {longest_target[\"speaker_id\"]}-{longest_target[\"speaker_chapter\"]}')\n",
    "      \n",
    "longest_input = max(vctk_train, key=lambda x: x['feature'].size(0))\n",
    "print(f'longest_input: T={longest_input[\"feature\"].size(0)} in {longest_input[\"speaker_id\"]}-{longest_input[\"speaker_chapter\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    num_batch = len(batch)\n",
    "    # Sort batch in decrease order. why?\n",
    "    batch = sorted(batch, key=lambda x: x['feature'].size(0), reverse=True)\n",
    "    max_seq_len, feature_dim = batch[0]['feature'].size()\n",
    "    \n",
    "    sex_label = torch.tensor([x['sex_label'] for x in batch])\n",
    "    \n",
    "    feature_ret = torch.zeros(num_batch, 1, feature_dim, max_seq_len)\n",
    "    seq_lengths = torch.IntTensor(num_batch)\n",
    "    targets = []\n",
    "    target_lengths = torch.IntTensor(num_batch)\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        feature = sample['feature'].t() #feature_dim, T\n",
    "        seq_len = feature.size(1)\n",
    "        seq_lengths[i] = seq_len\n",
    "        feature_ret[i][0].narrow(1, 0, seq_len).copy_(feature)\n",
    "        \n",
    "        txt = sample['txt']\n",
    "        targets.extend(txt)\n",
    "        target_lengths[i] = len(txt)\n",
    "        \n",
    "    return {'feature': feature_ret, \n",
    "            'sex_label': sex_label, \n",
    "            'targets': torch.tensor(targets),\n",
    "            'seq_lengths': seq_lengths,\n",
    "            'target_lengths': target_lengths}\n",
    "\n",
    "dataloader_tmp = DataLoader(dataset=vctk_train, batch_size = 4, collate_fn=collate_fn)\n",
    "iter_tmp = iter(dataloader_tmp)\n",
    "for i in range(2):\n",
    "    sample = next(iter_tmp)\n",
    "    print(f'feature: {sample[\"feature\"].size()}')\n",
    "    print(f'sex_label: {sample[\"sex_label\"].size()}')\n",
    "    print(sample[\"targets\"].size())\n",
    "    print(sample[\"seq_lengths\"].size())\n",
    "    print(sample[\"target_lengths\"].size())\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset=vctk_train, batch_size=config.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "dataloader_dev = DataLoader(dataset=vctk_dev, batch_size=config.batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SexClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SexClassifier, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = SpeakerClassifier(n_class=2)\n",
    "    \n",
    "    def forward(self, feature):\n",
    "        feature = feature.permute(0, 2, 1)\n",
    "        encoded = self.encoder(feature)\n",
    "        logits = self.decoder(encoded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acc(object):\n",
    "    def __init__(self, name):\n",
    "        self.count = 0\n",
    "        self.correct = 0\n",
    "        self.name = name\n",
    "    def update(self, predict, expect):\n",
    "        self.count += predict.size(0)\n",
    "        self.correct += int((predict == expect).float().sum())\n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.correct = 0\n",
    "    def get_acc(self):\n",
    "        return self.correct / self.count if self.count > 0 else 0\n",
    "    def __str__(self):\n",
    "        return f'acc {name}:{format(self.get_acc(), \".2f\")}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, dataloader_train, dataloader_dev, opt):\n",
    "        self.model = model.cuda()\n",
    "    def train(self):\n",
    "        pass\n",
    "    def train_epoch(self):\n",
    "        pass\n",
    "    def validate(self):\n",
    "        pass\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, dataloader_train, dataloader_dev):\n",
    "        self.model = model.cuda()\n",
    "        \n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.iter_train = iter(self.dataloader_train)\n",
    "        self.dataloader_dev = dataloader_dev\n",
    "        \n",
    "        self.opt = optim.Adam(self.model.parameters(), lr=config.lr, betas=(0.5, 0.9))\n",
    "        \n",
    "        self.acc_train = Acc('train')\n",
    "        self.acc_dev = Acc('dev')\n",
    "\n",
    "        train_log = SummaryWriter(serialization_dir / \"log\" / \"train\")\n",
    "        validation_log = SummaryWriter(serialization_dir / \"log\" / \"validation\")\n",
    "        self.tensorboard = TensorboardWriter(train_log, validation_log)\n",
    "        \n",
    "        self.global_steps = 0\n",
    "        self.epochs = 0\n",
    "        \n",
    "        self.train_loss = 0\n",
    "        self.cur_step = 0\n",
    "        self.dev_step = 0\n",
    "    \n",
    "    def reset_epoch(self):\n",
    "        self.acc_train.reset()\n",
    "        self.acc_dev.reset()\n",
    "        self.iter_train = iter(self.dataloader_train)\n",
    "        self.train_loss = 0\n",
    "        self.cur_step = 0\n",
    "\n",
    "    def get_train_loss(self):\n",
    "        return 0 if self.cur_step == 0 else float(self.train_loss / self.cur_step)\n",
    "    \n",
    "    def get_dev_loss(self):\n",
    "        return 0 if self.dev_step == 0 else float(self.dev_loss / self.dev_step)\n",
    "    \n",
    "    def show_train(self):\n",
    "        logger.info(f'{self.epochs}/{self.cur_step}： loss= {format(self.get_train_loss(), \".2f\")} acc= {self.acc_train}')\n",
    "   \n",
    "    def show_dev(self):\n",
    "        logger.info(f'validation: step= {self.global_steps} loss= {format(self.get_dev_loss(), \".2f\")} acc= {self.acc_dev}')\n",
    "                    \n",
    "    def train(self):\n",
    "        model = self.model\n",
    "        # Reset all\n",
    "        self.reset_epoch()\n",
    "        # Train\n",
    "        for self.global_steps in tqdm(range(config.train_steps), desc='train_steps'):\n",
    "            # Train model\n",
    "            model.train()\n",
    "            try:\n",
    "                data = next(self.iter_train) \n",
    "            except StopIteration:\n",
    "                self.reset_epoch()\n",
    "                data = next(self.iter_train)\n",
    "                self.epochs += 1\n",
    "            \n",
    "            self.cur_step += 1\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            feature = data['feature'].cuda()\n",
    "            sex_label = data['sex_label'].cuda()\n",
    "\n",
    "            logits = model(feature)\n",
    "\n",
    "            predict = logits.detach().cpu().argmax(dim=1)\n",
    "            self.acc_train.update(predict, data['sex_label'])\n",
    "\n",
    "            loss = F.cross_entropy(logits, sex_label)\n",
    "\n",
    "            self.train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            \n",
    "            self.show_train()\n",
    "            if (self.global_steps+1) % config.summary_interval == 0:\n",
    "                self.tensorboard.add_train_scalar(\"loss/loss_train\", self.get_train_loss(), self.global_steps)\n",
    "                self.tensorboard.add_train_scalar(\"acc/acc_train\", self.acc_train.get_acc(), self.global_steps)\n",
    "\n",
    "            if (self.global_steps+1) % config.valid_steps == 0:\n",
    "                self.validate()\n",
    "    \n",
    "    def validate(self):\n",
    "        model.eval()\n",
    "        self.dev_loss = 0\n",
    "        self.dev_step = 0\n",
    "        self.acc_dev.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_dev in tqdm(self.dataloader_dev, desc='validation'):\n",
    "                self.dev_step += 1\n",
    "                logits = model(data_dev['feature'].cuda()).cpu()\n",
    "                predict = logits.argmax(dim=1)\n",
    "                self.acc_dev.update(predict, data_dev['sex_label'])\n",
    "\n",
    "                loss = F.cross_entropy(logits, data_dev['sex_label'])\n",
    "                self.dev_loss += loss.item()\n",
    "\n",
    "                if DEBUG:\n",
    "                    break\n",
    "        \n",
    "        self.show_dev()\n",
    "        self.tensorboard.add_validation_scalar(\"loss/loss_dev\", self.get_dev_loss(), self.global_steps)\n",
    "        self.tensorboard.add_validation_scalar(\"acc/acc_dev\", self.acc_dev.get_acc(), self.global_steps)\n",
    "\n",
    "model = SexClassifier()\n",
    "trainer = Trainer(model, dataloader_train, dataloader_dev)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
